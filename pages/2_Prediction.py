import streamlit as stimport streamlit.components.v1 as componentsimport pandas as pdimport matplotlib as pltimport numpy as npfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curvefrom sklearn.ensemble import RandomForestClassifierfrom sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, learning_curve, train_test_splitfrom scipy.stats import randintst.set_page_config(page_title="SE CSM Platform - Prediction", page_icon='sporteasy_logo.png', layout="wide")data = pd.read_parquet('data_health_score.parquet', engine='pyarrow')x_months = 8# Filtrer les clients avec au moins x_months mois de donnéesdata_filtre = data.loc[data.date_start <= '2025-05-01']data_filtre = data_filtre.groupby('club_id').filter(lambda x: len(x) >= x_months)# Sélectionnez les x_months derniers mois pour chaque clientdata_x_months_derniers_mois = data_filtre.groupby('club_id').tail(x_months)data_x_months_derniers_moisclients_stats_list = []critères = data_x_months_derniers_mois[['login','game_score','attendance','collections','database','message_system','team_message','activation']]for club_id, groupe in data_x_months_derniers_mois.groupby('club_id'):    churn_value = groupe['churn'].iloc[-1]      mois = np.arange(len(groupe)).reshape(-1, 1)      stats_client = {'club_id': club_id, 'churn': churn_value}    for critère in critères:        valeurs = groupe[critère].values        stats_client[f'{critère}_moyenne'] = np.mean(valeurs)        stats_client[f'{critère}_médiane'] = np.median(valeurs)        stats_client[f'{critère}_ecart-type'] = np.std(valeurs)        model = LinearRegression().fit(mois, valeurs)        stats_client[f'{critère}_pente'] = model.coef_[0]        stats_client[f'{critère}_amplitude'] = np.max(valeurs)-np.min(valeurs)    clients_stats_list.append(stats_client)res = pd.DataFrame(clients_stats_list)club_static_features = pd.read_excel('clubs_static_features.xlsx')club_static_features = club_static_features.drop_duplicates(subset='club_id')res = pd.merge(res, club_static_features, on='club_id', how='left')renew_jan = pd.read_excel('target_renew_jan_25.xlsx')['club_id']renew_feb = pd.read_excel('target_renew_feb_25.xlsx')['club_id']renew_mars = pd.read_excel('target_renew_mars_25.xlsx')['club_id']renew_apr = pd.read_excel('target_renew_apr_25.xlsx')['club_id']renew_may = pd.read_excel('target_renew_may_25.xlsx')['club_id']renew_jun = pd.read_excel('target_renew_june_25.xlsx')['club_id']# new train/test split by quarter#to_renew = pd.concat([renew_jan,renew_feb, renew_mars], axis=0)to_renew = renew_jun# train/test splitX_test = res[res['club_id'].isin(set(to_renew))]y_test = res[res['club_id'].isin(set(to_renew))].churnX_train = res[res['club_id'].isin(set(res.club_id) - set(to_renew))]y_train = res[res['club_id'].isin(set(res.club_id) - set(to_renew))].churn#X_train = X_train.loc[X_train.club_country_id.isin(['FR','BE','CH'])]#y_train_aux = res[res['club_id'].isin(set(res.club_id) - set(to_renew))]#y_train = y_train_aux.loc[y_train_aux.club_country_id.isin(['FR','BE','CH'])].churn#X_test = X_test.loc[X_train.club_country_id.isin(['FR','BE','CH'])]#y_test_aux = res[res['club_id'].isin(set(res.club_id) - set(to_renew))]#y_test = y_test_aux.loc[y_test_aux.club_country_id.isin(['FR','BE','CH'])].churn# Balance between train and test datasetnum_churn_train = sum(X_train.churn == 1)num_churn_test = sum(X_test.churn == 1)explode = (0.05,0)fig, (ax1,ax2) = plt.subplots(1,2,figsize=(10,5))ax1.pie([num_churn_train,len(X_train)-num_churn_train], explode=explode, autopct='%1.1f%%',)ax1.set_title('train', fontweight="bold")ax2.pie([num_churn_test,len(X_test)-num_churn_test], explode=explode, autopct='%1.1f%%',)ax2.set_title('test', fontweight="bold")fig.legend([ax1, ax2], labels=['1 - has churned','0 - has not churned'])# traitement des features qualitativesX_train['country'] = X_train['club_country_id'].apply(lambda x: 1 if x in ['FR', 'CH', 'BE'] else 0)X_train['sport'] = X_train['club_sport_name'].apply(lambda x: 1 if x in ['football', 'rugby'] else 0)X_train['price'] = X_train['club_total_price'].apply(lambda x: float(x.replace(',', '.')))X_test['country'] = X_test['club_country_id'].apply(lambda x: 1 if x in ['FR', 'CH', 'BE'] else 0)X_test['sport'] = X_test['club_sport_name'].apply(lambda x: 1 if x in ['football', 'rugby'] else 0)X_test['price'] = X_test['club_total_price'].apply(lambda x: float(x.replace(',', '.')))X_train_rf = X_train.drop(['club_id','churn','club_sport_name','club_total_price','club_sales_person_name','club_country_id'],axis=1)X_test_rf = X_test.drop(['club_id','churn','club_sport_name','club_total_price','club_sales_person_name','club_country_id'],axis=1)#X_train_rf = X_train.drop(['club_id','churn'],axis=1)#X_test_rf = X_test.drop(['club_id','churn'],axis=1)rf = RandomForestClassifier()rf.fit(X_train_rf, y_train)rfpred = rf.predict(X_test_rf)print(classification_report(y_test, rfpred))ypred = rfpredmodel = rfprint ('Confusion Matrix:')print(confusion_matrix(y_test, ypred))print('\nAccuracy:', accuracy_score(y_test, ypred))print("Overall Precision:",precision_score(y_test, ypred))print("Overall Recall:",recall_score(y_test, ypred))print("Overall f1-score:", f1_score(y_test, ypred))auc = roc_auc_score(y_test,ypred)plt.show()feature_importances = model.feature_importances_features = X_train_rf.columnsimportance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})importance_df = importance_df.sort_values(by='Importance', ascending=False)plt.figure(figsize=(10, 6))plt.barh(importance_df['Feature'], importance_df['Importance'])plt.xlabel('Importance')plt.title('Importance des Caractéristiques')plt.gca().invert_yaxis()  # Inverser pour avoir la caractéristique la plus importante en hautplt.show()params = {    'n_estimators': randint(1, 200),    'max_depth': randint(1, 20),    'min_samples_split': randint(2, 11),    'min_samples_leaf': randint(1, 11),    'bootstrap': [True, False]}rfnew = RandomForestClassifier()search = RandomizedSearchCV(rfnew, param_distributions=params, random_state=123, n_iter=100, cv=3, verbose=2, n_jobs=-1)search.fit(X_train_rf, y_train)best_params = search.best_params_myrf = search.best_estimator_thisypred = myrf.predict(X_test_rf)print(classification_report(y_test, thisypred))myrf = RandomForestClassifier(bootstrap=best_params['bootstrap'],                              max_depth=best_params['max_depth'],                              min_samples_leaf=best_params['min_samples_leaf'],                              min_samples_split=best_params['min_samples_split'],                              n_estimators=best_params['n_estimators'])myrf.fit(X_train_rf, y_train)thisypred = myrf.predict(X_test_rf)ypred = thisypredmodel = myrfprint ('Confusion Matrix:')print(confusion_matrix(y_test, ypred))print('Accuracy:', accuracy_score(y_test, ypred))print("Overall Precision:",precision_score(y_test, ypred))print("Overall Recall:",recall_score(y_test, ypred))auc = roc_auc_score(y_test,ypred)print("AUC:", auc)plt.show()feature_importances = model.feature_importances_features = X_train_rf.columnsimportance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})importance_df = importance_df.sort_values(by='Importance', ascending=False)plt.figure(figsize=(10, 6))plt.barh(importance_df['Feature'], importance_df['Importance'])plt.xlabel('Importance')plt.title('Importance des Caractéristiques')plt.gca().invert_yaxis()  # Inverser pour avoir la caractéristique la plus importante en hautplt.show()